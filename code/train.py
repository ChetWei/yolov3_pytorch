# -*- coding: utf-8 -*-
import argparse
import torch

from tqdm import tqdm
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader

from Yolo3Body import YOLOV3
from config import anchors_mask_list
from utils.datasets import ListDataSet
from utils.loss import compute_loss
from utils.util import weights_init, get_lr, save_model
from utils.augmentations import AUGMENTATION_TRANSFORMS


def create_dataloader(label_path, input_shape, batch_size, num_workers):
    dataset = ListDataSet(labels_file=label_path, input_shape=input_shape, transform=AUGMENTATION_TRANSFORMS)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=dataset.collate_fn
    )
    return dataloader, len(dataset)


def train_one_epoch(model, dataloader, optimizer, device, epoch, epoches, epoch_steps, writer):
    """
    è®­ç»ƒä¸€ä¸ªä¸–ä»£
    :param model: è®­ç»ƒçš„æ¨¡å‹
    :param dataloader: æ•°æ®åŠ è½½å™¨
    :param optimizer: ä¼˜åŒ–å™¨
    :param device: è®¾å¤‡
    :param epoch: å½“å‰è®­ç»ƒçš„epoch
    :param epoches: ç´¯è®¡è®­ç»ƒå¤šå°‘ä¸ªepoch
    :param epoch_steps:  æ¯ä¸ªepochæœ‰å¤šå°‘ä¸ªbatchæ­¥
    :param loss : æŸå¤±å‡½æ•°
    :param writer:  tensorboardå†™å…¥å™¨
    :return:
    """
    model.train()
    total_loss = 0
    with tqdm(total=epoch_steps, desc=f'Epoch {epoch + 1}/{epoches}', postfix=dict, mininterval=0.3) as pbar:
        for step, (imgs, targets) in enumerate(dataloader):
            imgs = imgs.to(device, non_blocking=True)
            targets = targets.to(device)
            # æ¢¯åº¦æ¸…é›¶
            optimizer.zero_grad()
            # å‘å‰ä¼ æ’­
            outputs = model(imgs)
            # è®¡ç®—æŸå¤±
            loss, loss_components = compute_loss(outputs, targets, model)
            # åå‘ä¼ æ’­
            loss.backward()
            # ä¼˜åŒ–å‚æ•°
            optimizer.step()

            total_loss += loss.item()
            # è®¡ç®—å½“å‰epochçš„å¹³å‡æŸå¤±
            mean_loss = total_loss / (step + 1)
            writer.add_scalar("loss/step", mean_loss, (step + 1) * (epoch + 1))
            writer.add_scalar("learning_rate/step", get_lr(optimizer), (step + 1) * (epoch + 1))
            info = "loss_v={},loss_mean={},lr={}".format(round(loss.item(), 6), round(mean_loss, 6), get_lr(optimizer))
            pbar.set_postfix_str(info)
            pbar.update(1)


def str2bool(v):
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description="Trains the YOLO3 model.")
    parser.add_argument('--cuda_id', type=int, default=0, help="ä½¿ç”¨çš„gpu")
    parser.add_argument('--pre_trained', type=str2bool, default=False, help="ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹")
    parser.add_argument("--weight_path", type=str, default="/Users/weimingan/work/weights/yolov3_voc_500.pth", help="é¢„è®­ç»ƒæƒé‡è·¯å¾„")
    parser.add_argument("--weight_dir", type=str, default="../weights", help="æ¨¡å‹æƒé‡ä¿å­˜ç›®å½•")
    parser.add_argument('--model_name', type=str, default="yolov3_voc", help="ä¿å­˜æ¨¡å‹åç§°")
    parser.add_argument("--save_per_epoch", type=int, default=100, help="æ¯å¤šå°‘è½®ä¿å­˜ä¸€æ¬¡æƒé‡")
    parser.add_argument("--logdir", type=str, default="./logs_voc", help="tensorboardä¿å­˜ç›®å½•")
    parser.add_argument('--label_path', type=str, default="../data/annotation/voc2007_train_local.txt", help="è®¾ç½®labelæ–‡ä»¶çš„è·¯å¾„")
    parser.add_argument("--num_classes", type=int, default=20, help="è®­ç»ƒæ•°æ®é›†çš„ç±»åˆ«ä¸ªæ•°")

    parser.add_argument('--freeze', type=str2bool, default=False, help="æ˜¯å¦å†»ç»“éª¨å¹²ç½‘ç»œ")

    parser.add_argument('--cosine_lr', type=str2bool, default=True, help="ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç­–ç•¥")
    parser.add_argument('--batch_size', type=int, default=4, help="batchçš„å¤§å°")
    parser.add_argument('--lr', type=float, default=0.00001, help="å­¦ä¹ ç‡")
    parser.add_argument('--decay', type=float, default=0.00005, help="decay")
    parser.add_argument('--input_shape', type=list, default=[416, 416], help="è¾“å…¥å›¾ç‰‡çš„å°ºå¯¸ w h")
    parser.add_argument("--epochs", type=int, default=500, help="è®­ç»ƒè½®æ¬¡")
    parser.add_argument('--num_workers', type=int, default=4, help="åŠ è½½æ•°æ®è¿›ç¨‹æ•°é‡")

    #========æ£€æµ‹æ—¶å€™ä½¿ç”¨==========
    parser.add_argument("--iou_thres", type=float, default=0.5,
                        help="Evaluation: IOU threshold required to qualify as detected")
    parser.add_argument("--conf_thres", type=float, default=0.1, help="Evaluation: Object confidence threshold")
    parser.add_argument("--nms_thres", type=float, default=0.5,
                        help="Evaluation: IOU threshold for non-maximum suppression")

    args = parser.parse_args()
    print(f"Command line arguments: {args}")

    # tensorboard log
    writer = SummaryWriter(log_dir=args.logdir)

    # é€‰æ‹©ä½¿ç”¨çš„è®¾å¤‡
    cuda_name = 'cuda:' + str(args.cuda_id)
    device = torch.device(cuda_name if torch.cuda.is_available() else 'cpu')

    # åŠ è½½æ¨¡å‹
    model = YOLOV3(args.num_classes, anchors_mask_list, img_size=args.input_shape[0]).to(device)
    # æƒé‡å‚æ•°å›ºå®šåˆå§‹åŒ–
    if args.pre_trained:
        model.load_state_dict(torch.load(args.weight_path, map_location=device))
    else:
        weights_init(model)

    # æ˜¯å¦å†»ç»“éª¨å¹²ç½‘ç»œ
    for param in model.backbone.parameters():
        param.requires_grad = not args.freeze

    # ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.decay)
    if args.cosine_lr:
        # ä½™å¼¦é€€ğŸ”¥
        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-5)
    else:
        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.94)

    dataloader, epoch_steps = create_dataloader(args.label_path, args.input_shape, batch_size=args.batch_size,
                                                num_workers=args.num_workers)

    for epoch in range(args.epochs):
        train_one_epoch(model, dataloader, optimizer, device, epoch, args.epochs, epoch_steps // args.batch_size,
                        writer)
        lr_scheduler.step()

        if ((epoch + 1) % args.save_per_epoch == 0):
            save_model(model, args.model_name, (epoch + 1), weights_dir=args.weight_dir)
